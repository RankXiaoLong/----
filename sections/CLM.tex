\chapter{古典线性回归模型}

在引论中，我们推出了满足凯恩斯条件的消费函数与收入有关的一个最普通模型：$ \mathrm{C = \alpha + \beta X +\varepsilon} $
其中$ \alpha > 0 , \, 0 < \beta <1$ ，$ \varepsilon$是一个随机扰动。这是一个标准的古典线性回归模型。假如我们得到如下\fref{tab:4.1} 的数据

\begin{table}[htb!]
    \centering
    \setlength{\tabcolsep}{3em}
    \caption{可支配个人收入和个人消费支出}
        \begin{tabular}{c  c  c}
            \toprule
            年份 & 可支配收入 & 个人消费 \\
            \midrule
            1971 & 779.2 & 696.8 \\
            1972 & 810.3 & 737.1 \\
            1973 & 864.7 & 767.9 \\
            1974 & 857.5 & 762.8 \\
            1975 & 847.9 & 779.4 \\
            1976 & 906.8 & 823.1 \\
            1977 & 942.9 & 964.3 \\
            1978 & 988.8 & 903.2 \\
            1979 & 1015.7 & 927.6 \\
            \bottomrule     
        \end{tabular} 
        \label{tab:4.1}     
\end{table}

\section{线性回归模型及其假定}

一般地，被估计模型具有如下形式：
$$
y_{i}=\alpha+\beta x_{i}+\varepsilon_{i}, \quad i=1, \cdots, n
$$

其中 $\boldsymbol{y}$ 是因变量或称为被解释变量， $\boldsymbol{ x} $是自变量或称为解释变量， 
$i$ 标志 $n$ 个样本观测值中的一个。这个形式一般被称作 $\boldsymbol{y}$ 对 $\boldsymbol{x}$ 的总体线性回归模型。
在此背景下， $\boldsymbol{y}$ 称为被回归量， $\boldsymbol{x}$ 称为回归量

\subsection{构成古典线性回归模型的一组基本假设}
\begin{enumerate}[1)]
    \item 函数形式： $y_{i}=\alpha+\beta x_{i}+\varepsilon_{i}, \quad i=1, \cdots, n$。
    \item 干扰项的零均值：对所有 $i$，有：$\mathbb{E}\left[\varepsilon_{i}\right] = 0 $。
    \item 同方差性：对所有 $i$ ，有：$\operatorname{Var}\left[\varepsilon_{i}\right]=\sigma^{2}$,且$ \sigma^{2}$ 是一个常数。
    \item 无自相关：对所有 $i \neq j$，则 $ \operatorname{Cov}\left[\varepsilon_{i},\varepsilon_{j}\right] = 0 $。
    \item 回归量和干扰项的非相关：对所有 $i$ 和 $j $有 $ \operatorname{Cov}\left[x_{i},\varepsilon_{j}\right] = 0 $。
    \item 正态性：对所有 $\varepsilon_{i}$， $\varepsilon_{j}$ 满足正态分布 $ \operatorname{N}\left[0,\sigma \right]  $。
\end{enumerate}

\subsection{模型假定的几点说明 }

{\heiti 1. 函数形式及其线性模型的转换}

具有一般形式:
$$ f\left(y_{i}\right)=\alpha+\beta g\left(x_{i}\right)+\varepsilon_{i} $$

对任何形式的 $g(\boldsymbol{x})$都符合我们关于线性模型的定义。
\begin{myexample}[一个常用的函数形式是对数线性模型]
    $$ {\boldsymbol{ y }} = \boldsymbol{A} {\boldsymbol{ x} } ^{\beta}$$

    取对数得：$ \ln \boldsymbol{y} =\alpha+\beta \ln \boldsymbol{x} \quad (\alpha=\ln \boldsymbol{A}) $
    这被称作不变弹性形式。在这个方程中， $y$ 对于 $x$ 的变化的弹性是:
    $$ \eta=\frac{d \boldsymbol{y} / \boldsymbol{y}}{d \boldsymbol{x} / \boldsymbol{x}}=\frac{d \ln \boldsymbol{y}}{d \ln \boldsymbol{x}}=\beta $$

    它不随 $x$ 而变化。与之相反，线性模型的弹性是：
    $$\eta=\left(\frac{d \boldsymbol{y}}{d \boldsymbol{x}} / \frac{\boldsymbol{y}}{\boldsymbol{x}}\right)
         =\left(\frac{\boldsymbol{x}}{\alpha+\beta \boldsymbol{x}} \right)\left(\frac{d \boldsymbol{y} }{d \boldsymbol{x}}\right)
         =\frac{\beta \boldsymbol{x}}{\alpha+\beta \boldsymbol{x}}$$

    对数线性模型通常用来估计需求函数和生产函数，尽管线性模型具有巨大的灵活性，但在实际中存在着大量的非线性模型的形式。任何变换也不能将转化为线性回归模型。
    $$ \boldsymbol{y}=\alpha+\frac{1}{\beta+\boldsymbol{x} }  \ \ \&  \ \ y=\alpha+\beta \boldsymbol{x}^{v} \quad(0<v<1)$$
\end{myexample}

{\heiti 2. 回归量}

对于回归量即解释变量我们有两种处理方法，第一种将 $X$ 设定为非随机变量，第二种方法将 $X$ 设定为随机变量。

1) 当 $X $为非随机变量

    $x_i$ 的值在 $y_i$ 的概率分布中是已知的常数。这条假定暗示$ y_i $的每一个值都是一个概率分布的观察值，这个概率分布具有均值和方差。
    \begin{align*}
   \mathbb{E}\left[y_{i} \mid x_{i}\right] & = E\left[\alpha+\beta x_{i}+\varepsilon_{i}\right]
   =\alpha+\beta x_{i} + \mathbb{E}\left[\varepsilon_{i}\right]=\alpha+\beta x_{i} \\
   \operatorname{Var}\left[y_{i} \mid x_{i}\right] & =\operatorname{Var}
   \left[\alpha+\beta x_{i}+\varepsilon_{i}\right]=\operatorname{Var}\left[\varepsilon_{i}\right]=\sigma^{2}
   \end{align*}

   此外，有必要假定，对 $n \leq 1$
   $$\left(\frac{1}{n}\right) S_{x x}=\left(\frac{1}{n}\right) \sum_{i}\left(x_{i}-\bar{x}\right)^{2}$$

    是一个有限正数，这个假定被称作识别条件，若 $x_i$ 没有任何变化，我们所有的观测值将落在一条垂直线上，我们的观测数据将不允许我们作出关于回归$\alpha +\beta \boldsymbol{X} $ 的任何推断。
    这个识别条件等同于子样的极差
   $\max \left(X_{1}, \cdots, X_{n}\right)-\min \left(X_{1}, \cdots, X_{n}\right) \neq 0$

2) 当 $ X $为随机变量

 若 $ {\boldsymbol{x}} $ 被当作一个随机变量，则假定 1 成为一个对 $ {\boldsymbol{y} }$ 和 $ { \boldsymbol{x} }$ 的联合分布的陈述。 我们就用条件期望和方差来处理。

 \subsection{随机干扰项}

 如果干扰项不是零均值，即$ \mathbb{E}\left[\varepsilon_{\mathrm{i}}\right]=\mu $ ， 对所有的$ i$，
 则 $ \alpha+\beta x+\varepsilon_{\mathrm{i}} $ 等同于$  (\alpha+\mu) +\beta x+\left(\varepsilon_{i}-\mu\right) $，
  令 $\alpha^{\prime}=\alpha+\mu $ 及 $ \varepsilon_{i}^{\prime}=\varepsilon_{i}-\mu $  可得到模型， 此模型满足我们原始模型的要求。 

 观测值中的随机部分假定是不相关的：$ \mathbb{E} \left[ \varepsilon_{i} ,\varepsilon_{j} \right]=0$， 对所有 $i$ 不等于 $j$ ，这被称为非自相关。

 \section{最小二乘法}
 \subsection{最小二乘系数}
 
 总体回归是  $\mathbb{E}\left[y_{i} \mid x_{i}\right]=\alpha+\beta x_{i}$ , 
 而我们对$ \mathbb{E}\left[y_{i} \mid x_{i}\right]$ 的估计记作
 $$ \hat{y}_{i}=a+b x_{i} $$  

 和第 $ i $ 的数据点相联系的干扰项是:
$$ \varepsilon_{i}=y_{i}-\alpha-\beta x_{i} $$

对 a 和 b 的任何值，我们用残差:
$$ e_{i}=y_{i}-a-b x_{i} $$

来估计$ \varepsilon_{i} $ ， 从这些定义可知： 
$$ y_{i}=\alpha+\beta x_{i}+\varepsilon_{i}  = a+b x_{i}+e_{i}$$

对任何一对值 a 和 b，残差平方和是：
$$ \sum_{i} e_{i}^{2}=\sum_{i}\left(y_{i}-a-b x_{i}\right)^{2} $$

最小二乘法系数就是使这个拟合标准达到最小的 a 和 b 的值。最小化的一阶条件是
\begin{align}
    \frac{\partial\left(\Sigma_{i} e_{i}^{2}\right)}{\partial a} & = \sum_{i} 2\left(y_{i}-a-b x_{i}\right)(-1) \notag  \\ 
    & = -2 \sum\left(y_{i}-a-b x_{i}\right)=0  \notag  \\
    \frac{\partial\left(\Sigma_{i} e_{i}^{2}\right)}{\partial b} &=\sum_{i} 2\left(y_{i}-a-b x_{i}\right)\left(-x_{1}\right)=0 \notag  \\
    &=-2 \sum_{i} x_{i}\left(y_{i}-a-b x_{i}\right)=0  \notag 
\end{align} 
    
将上两式展开合并同类项后得到正规方程组:
\begin{align}
    \sum_{i} y_{i} & = n a +\left(\sum_{i} x_{i}\right) b  \label{eq 4.2.1} \\
    \sum x_{i} y_{i} & = \left(\sum_{i} x_{i}\right) a+\left(\sum_{i} x_{i}^{2}\right) b \label{eq 4.2.2}
\end{align}

\eqref{eq 4.2.1} 暗示$ \sum_{i=1}^{n} e_{i}=0 $  ，而 \eqref{eq 4.2.2}  暗示 $ \sum_{i} X_{i} e_{i}=0 $

为了得到解，我们首先用 $n$ 除 \eqref{eq 4.2.1} ， 结果是$ \bar{y}=a+b \bar{x} $。最小二乘回归线通过均值点。现在分离 a：
\begin{equation}
    a=\bar{y}-b \bar{x} \label{eq 4.2.3}
\end{equation}

有了 $a$ 后，我们可以求解\eqref{eq 4.2.1}得到 $b$。首先，$\Sigma_{i} x_{i}=n \bar{x}$ 。将此和\eqref{eq 4.2.3}代入\eqref{eq 4.2.2}并重
新安排各项。
$$ \sum_{i} x_{i} y_{i}-n \overline{x y}=b\left(\sum_{i} x_{i}^{2}-n \bar{x}^{2}\right) $$

或
$$b=\frac{\Sigma_{i} x_{i} y_{i}-n \overline{x y}}{\sum_{i} x_{i}^{2}-n \bar{x}^{2}}=\frac{\sum_{i}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i}\left(x_{i}-\bar{x}\right)^{2}}$$

最小的残差平方和，对 a 和 b 的二阶微分矩阵是：
$$\left[\begin{array}{cc}
    \partial^{2}\left(\Sigma_{i} e_{i}^{2}\right) / \partial a^{2} & \partial^{2}\left(\Sigma_{i} e_{i}^{2}\right) / \partial a \partial b \\
    \partial^{2}\left(\Sigma_{i} e_{i}^{2}\right) / \partial b \partial a & \partial^{2}\left(\Sigma_{i} e_{i}^{2}\right) / \partial b^{2}
    \end{array}\right]=\left[\begin{array}{cc}
    2 n & 2 \Sigma_{i} x_{i} \\
    2 \Sigma_{i} x_{i} & 2 \Sigma_{i} x_{i}^{2}
\end{array}\right]$$

我们必须表明这是一个正定矩阵，两个对角元素永远为正，所以仅需证明行列式为正， $(4 n) \Sigma_{i} x_{i}^{2}-4\left(\Sigma_{i} x_{i}\right)^{2}$, 但$\Sigma_{i} x_{i}=n \bar{x}$， 所以行列式为
$$ 4 n\left(\sum_{i} x_{i}^{2}-n \bar{x}^{2}\right)=4 n\left(\sum_{i}\left(x_{i}-\bar{x}\right)^{2}\right)$$ 

由识别条件得知这是一个正值。这样 a 和 b 是平方和的最小化因子。
 \subsection{回归拟合的评价}

1) 回归量 x 是非随机变量

总变差是离差的平方和：
    \begin{align*}
    S S T & = \sum_{i}\left(y_{i}-\bar{y}\right)^{2}  = \sum_{i}\left(\hat{y}_{i}-\bar{y}\right)^{2}+\sum_{i} e_{i}^{2}+2 \sum e_{i} \hat{y}_{i} \\ 
    & = b^{2} \sum_{i}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i} e_{i}^{2}
    \end{align*} 

    第二个等式成立是因为：$\sum_{i=1}^{n} e_{i} \hat{y}_{i}=\sum_{i=1}^{n} e_{i}\left(a+b X_{i}\right)=a \sum e_{i}+b \sum_{i=1}^{n} e_{i} X_{i}=0$

    我们将其写作：
    $$ SST = SSR + SSE $$

    我们利用下式得到一个关于回归直线对数据拟合程度的度量:
    $$ R^{2}=\frac{S S R}{S S T} $$

    为了方便计算与分析，约定
    $$ \begin{array}{ll}
        S_{x x}=\sum\left(x_{i}-\bar{x}\right)^{2}, & S_{x}=\sqrt{S_{x x}} \\
        S_{y y}=\sum\left(x_{i}-\bar{x}\right)^{2}, & S_{y}=\sqrt{S_{y y}} \\
        S_{x y}=\sum_{i}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)
        \end{array} $$ 

    $x$ 和 $y$ 间的样本相关系数是 : $ r_{x y}=S_{x y} /\left(S_{x} S_{y}\right) $ ，利用b =$ b=S_{x y} / S_{x x} $ 我们得到 $r_{x y}=b /\left(S_{y} / S_{x}\right)$
    这表明回归的斜率和 $x$、 $y$ 间的相关系数具有相同的符号，而且
    $$ R^{2}=\frac{S S R}{S S T}=\frac{b^{2} S_{x x}}{S_{y y}}=r_{x y}^{2}  $$

    这进一步证明了我们利用 $R^{2}$ 作为回归模型拟合优劣指标的正确性。
 \subsection{方差分析表(ANOVA)}

 进一步研究回归平方和 SSR 与残差平方和 SSE，我们可以得到下面三个结论：
 \begin{enumerate}[a)]
    \item 在$ \beta = 0 $ 的假设条件下， 回归平方和 $\dfrac{S S R}{\sigma^{2}}$服从自由度为 1 的卡方分布  $ \chi ^{2}(1) $
    \item 残差平方和$\dfrac{S S E}{\sigma^{2}}$服从自由度为 n-2 的卡方分布 $\chi ^{2}(n-2)$
    \item 在$ \beta = 0 $ 的假设条件下， $ \dfrac{S S R / 1}{\operatorname{SSE} /(n-2)} $ 服从 $F(1,n-2)$分布。现在我们来证明这三个结论
 \end{enumerate} 

 \begin{myproof}
    \begin{align*}
        {\bf Proof (a)}: \quad  c_{i} & = \frac{x_{i}-\bar{x}}{S_{x x}} , \ \ b  = \frac{S_{x y}}{S_{x x}}  = \frac{\sum_{i}\left(x_{i}-\bar{x}\right) y_{i}}{S_{x x}}  = \sum_{i} c_{i}y_{i}\\
       \Longrightarrow & \sum_{i} c_{i}^{2}=\frac{1}{S_{x x}}  \ \  ,  \ \  Suppose \ \ \boldsymbol{C} =\left(c_{1}, c_{2} \cdots c_{n}\right)^{\prime} \\ 
       \Longrightarrow &  b=\boldsymbol{C^{\prime} Y}\ \  ,  \ \ b^{2}=\boldsymbol{ Y^{\prime} C C^{\prime} Y} \ \  , 
        \ \  S S R=b^{2} \sum\left(x_{i}-\bar{x}\right)^{2}=S_{x x} b^{2}=\boldsymbol{ Y^{\prime}} S_{x x} \boldsymbol{ C C^{\prime} Y }
    \end{align*} 

    可以验证$ S_{x x} \boldsymbol{C C^{\prime}} $是幂等矩阵。
    \begin{align*}
        S_{x x} \boldsymbol{C C^{\prime}} \cdot S_{x x} \boldsymbol{C C^{\prime}} =S_{x x}^{2} \boldsymbol{ C\left(C^{\prime} C\right) C} 
         =S_{x x} \boldsymbol{ C C^{\prime} } \\
        r\left(S_{x x} \boldsymbol{  C C^{\prime} } \right)=\operatorname{tr}\left(S_{x x} \boldsymbol{  C C^{\prime}}\right)=S_{x x} \sum_{i} c_{i}^{2}=1
    \end{align*}

    在$ \beta  = 0 $的假设条件下， $\dfrac{S S R}{\sigma^{2}}$ 才服从自由度为 1 的卡方分布 $ \chi^2(1) $ 

    {\bf Proof (b)}:因为 $ SST = \boldsymbol{YM_{0}Y} $及$ SST = SSR + SSE $ 
      $ \Longrightarrow  SSE = \boldsymbol{ Y^{\prime}} \left(\boldsymbol{ M_{0} } -S_{x x} \boldsymbol{ C C^{\prime}}\right) Y $

    已验证 $\boldsymbol{ M_{0 }}-S_{x x} \boldsymbol{ C C^{\prime}}$ 也是幂等矩阵, $\boldsymbol{ C^{\prime} i}=\boldsymbol{ i^{\prime} C}=\sum_{i} c_{i}=0$。
    \begin{align*}
        \left(\boldsymbol{ M_{0}} -S_{x x} \boldsymbol{ C C^{\prime}} \right)^{2} & 
        = \boldsymbol{ M_{0}}-S_{x x} \boldsymbol{ C C^{\prime} M_{0}} -S_{x x} \boldsymbol{  M_{0} C C^{\prime}} +S_{x x} \boldsymbol{ C C^{\prime} }\\
        & =\boldsymbol{ M_{0}}-S_{x x} \boldsymbol{ C C^{\prime} }+\frac{1}{n} 
         S_{x x} \boldsymbol{  C C^{\prime} i i^{\prime}} +\frac{1}{n} S_{x x} \boldsymbol{  i i^{\prime} C C^{\prime}} 
         =\boldsymbol{ M_{0}} -S_{x x} \boldsymbol{ C C^{\prime}} \\
        & \Longrightarrow r\left(\boldsymbol{ M_{0}} -S_{x x}\boldsymbol{ C C^{\prime} }\right)
        =\operatorname{tr}\left(\boldsymbol{  M_{0} } -S_{x x} \boldsymbol{  C C^{\prime}} \right)=n-1-S_{x x} \cdot \frac{1}{S_{x x}}=n-2
    \end{align*}

    因此$\dfrac{S S E}{\sigma^{2}} \sim x^{2}(n-2) $此结论成立不需要$ \beta  = 0 $的假设条件下，为什么？

    
        
    所以 $SSR$ 与 $SSE$ 是相互独立的统计量。从而， 在$ \beta  = 0 $的假设条件下，
     $ \frac{S S R / 1}{\operatorname{SSE} /(n-2)} $ 服从 $F(1,n-2)$分布，所以，可以用来作模型的整体检验的统计量。 
    \begin{align}
        {\bf Proof (c)}: \quad 
        S_{x x} \boldsymbol{ C C^{\prime}} \cdot \left(\boldsymbol{ M_{0}}-S_{x x} \boldsymbol{ C C^{\prime}}\right)  & 
         =  S_{x x} \boldsymbol{ C C^{\prime}} \cdot \left(\boldsymbol{ I} -\frac{1}{n} \boldsymbol{ i i^{\prime}}
         -S_{x x} \boldsymbol{  C C^{\prime} }\right) \notag \\
        & = S_{x x} \boldsymbol{ C C^{\prime}}-\frac{1}{n} S_{x x} \boldsymbol{ C C^{\prime} i i^{\prime}}-S_{x x} \boldsymbol{ C C^{\prime} }= 0 \notag
    \end{align}
 \end{myproof}

 \begin{table}[htb!]
    \centering
    \caption{方差分析表}
    \begin{tabular}{cccc}
        \hline 变差来源 & 变差 & 自由度 & 均方 \\
        \hline 回归 & $S S R=b^{2} S_{x x}$ & 1 & $\frac{S S R}{1}$ \\
        残差 & $S S T=\Sigma_{i} e_{i}^{2}$ & $n-2$ & $\frac{S S E}{n-2}$ \\
        总 & $S S T=S_{y y}$ & $n-1$ & $\frac{S_{y y}}{n-1}$  \vspace{0.5em} \\
         & $F[1, n-2]=\dfrac{S S R / 1}{S S E /(n-2)}$ & \\
        \hline
    \end{tabular}
    \label{tab 4.1}
 \end{table}

 概括这些计算的一个方便的途径是方差分析表，可总结在方差分析表\ref{tab 4.1} 中。

 2) 回归量 $X$ 是随机变量

 我们要利用方差分解公式:
    \begin{align}
        \operatorname{Var}(Y) &=\operatorname{Var}( \mathbb{E}(Y \mid X))+\mathbb{E}\left(\operatorname{Var}_{x}(Y \mid X)\right)  \notag \\
        &=\operatorname{Var}(\alpha+\beta X)+\mathbb{E} \left[\mathbb{E}(Y-\mathbb{E}(Y \mid X))^{2} \mid X\right] \notag  \\
        &=\beta^{2} \operatorname{Var}(X)+\mathbb{E}\left[\mathbb{E}(Y-\mathbb{E}(Y \mid X))^{2} \mid X\right] \notag  
    \end{align}

我们将它应用到子样空间里来，即
$$ \frac{1}{n} \sum_{i}\left(y_{i}-\bar{y}\right)^{2}=b^{2} \frac{1}{n} \sum_{i}\left(x_{i}-\bar{x}\right)^{2}+\frac{1}{n} \sum_{i} e_{i}^{2} $$

所以，两边去掉 $ 1/n $ 后得到：
$$ \sum_{i}\left(y_{i}-\bar{y}\right)^{2}=b^{2} \sum_{i}\left(x_{i}-\bar{x}\right)^{2}+\sum_{i} e_{i}^{2} $$

对消费和收入数据，方差分析表如下所示：
\begin{table}[htb!]
    \centering
    \caption{方差分析表}
    \begin{tabular}{cccc}
        \hline 变差来源 & 变差 & 自由度 & 均方 \\
        \hline 回归 & $64,435.15$ & 1 & $ 64,435.13$ \\
        残差 & $537.00$ &  $8$ & $67.124$ \\
        总 & $64,972.13$ & $9$ & $7,219.12$  \vspace{0.3em} \\
         & $F[1, 8]=\dfrac{64,435.13}{67.124} = 959.94$ &  \vspace{ 0.5em }\\
        \hline
    \end{tabular}
 \end{table}

我们得到了和把 $X$ 当成非随机变量时同样的结果，因此，方差分析表也是一样的。考虑消费函数的例子，这里 $C$ 是消费而 $X$ 是收入，我们得到:
$\bar{C}=793.43, \quad \bar{X}=879.24$ , $S_{C C}=64,972.12, \quad S_{X X}=67,192.44$ , $S_{X C}=65,799.34 $ 。 $ SST = 64,972.12$ ,
$ SSR = 64,435.13$ , $ SSE = 537.00 $ $ \Longrightarrow $ $ R^{2} = \dfrac{64,435.13}{64,972.12} = 0.99173$ 。显然，此回归提供了一个很好的拟合。


 另一个计算和通常 $R^{2}$ 相类似公式是：
 $$ R^{2}=1-\frac{\Sigma_{i} e_{i}^{2}}{S_{y y}} $$

 任何一个模型的残差都可用 $ y_{i} − \hat{y} $ 来计算。
 \section{最小二乘法估计量的统计特征}

 我们利用了最小二乘法，从纯粹的代数方法，求得所拟合的最小二乘系数 $a$ 和 $b$，从统计意义上来说，这个结果可以看作是对参数$ \alpha $和$ \beta $的一个估计（因为还存在着利用其他估计方法得到的估计）。我们现在对 $a$、 $b$ 的无偏性，有效性和精确度等统计特性作分析。

 我们所考虑的计量模型是：
 $$ y_{i}=\alpha+\beta x+\varepsilon_{i} $$

 $ \beta $ 的最小二乘估计是:
\begin{equation}
    b =\frac{S_{x y}}{S_{x x}}=\frac{\sum_{i}\left(x_{i}-\bar{x}\right) y_{i}}{S_{x x}} =\sum_{i} c_{i} y_{i}
    \label{eq 4.3.1}
\end{equation}

其中权数$ c_{i} $ 仅仅是$ x_{1},\cdots,x_{n}$ 的一个函数。
\begin{equation}
    c_{i}=\frac{x_{i}-\bar{x}}{S_{x x}}
    \label{eq 4.3.2}
\end{equation}

\subsection{b 是 \texorpdfstring{$\beta$}{β} 的无偏估计}  


将 $ y_{i}=\alpha+\beta x+\varepsilon_{i} $ 代入 \ref{eq 4.3.1}，我们得到
\begin{align}
    b &=\frac{\sum_{i}\left(x_{i}-\bar{x}\right)\left(\alpha+\beta x_{i}+\varepsilon_{i}\right)}{S_{x x}} \notag \\
    &=\frac{\alpha \Sigma_{i}\left(x_{i}-\bar{x}\right)}{S_{x x}}+\frac{\beta \Sigma_{i}\left(x_{i}-\bar{x}\right) x_{i}}{S_{x x}}+\frac{\sum_{i}\left(x_{i}-\bar{x}\right) \varepsilon_{i}}{S_{x x}} \label{eq 4.3.3} \\
    &=\beta+\sum_{i} c_{i} \varepsilon_{i} \notag
\end{align}

所以
\begin{equation}
    \mathbb{E}[b]=\beta+\mathbb{E}\left[\sum_{i} c_{i} \varepsilon_{i}\right]=\beta
\end{equation}

这是因为 $ \mathbb{E} \left[ \varepsilon_{i} \right] = 0 $。不论 $ \varepsilon $ 的分布如何，在我们其他假定下， $ b $ 是$ \beta $ 的一个无偏估计
量，利用 \ref{eq 4.3.3} 得到 $ b  $ 的样本方差
$$ \operatorname{Var}[b]=\operatorname{Var}[b-\beta]=\operatorname{Var}\left[\sum_{i} c_{i} \varepsilon_{i}\right] $$

特别要注意 $ b $ 的方差中的分母。 $ x $ 的变差越大（也就是 $ x $ 的采样范围越广），则这个方差越小。

\subsection{a 是 \texorpdfstring{$ \alpha $}{α} 的无偏估计} 

对于最小二乘截距 $ a $，我们有：
\begin{align*} 
    a & =\bar{y}-b \bar{x} \\ &=\frac{1}{n} \sum_{i} y_{i}-b \bar{x} 
        =\frac{1}{n} \sum_{i}\left(\alpha+\beta x_{i}+\varepsilon_{i}\right)-b \bar{x}
\end{align*}

利用\ref{eq 4.3.3}式并加以整理，我们有
$$ a-\alpha=\sum_{i} d_{i} \varepsilon_{i} $$

其中
$$ d_{i}=\left(\frac{1}{n}-\bar{x} c_{i}\right) $$

由于求和中每一项的期望都为0，所以 $a$ 也是$\alpha$ 的估计量无偏估计量。 $a$ 的样本方差就是
$\sum_{i} d_{i} \varepsilon_{i} $的方差，根据独立性有
$$ \operatorname{Var}[a]=\sum \sigma^{2} d_{i}^{2}=\sigma^{2}\left(\frac{1}{n}+\bar{x}^{2} \sum_{i} c_{i}^{2}\right)
       =\sigma^{2}\left[\frac{1}{n}+\frac{\bar{x}^{2}}{S_{\mu x}}\right] $$

（通过对括号中的项进行平方并利用 $ \sum_{i}c_{i} = 0 $ 的结果，可以得到上式中后一结果）。

{\subsection{a ,  b 估计量的协方差矩阵}  }

两个估计的协方差是:
\begin{align*}
    \operatorname{Cov}[a, b] & = \mathbb{E}[(a-\alpha)(b-\beta)]=\mathbb{E}\left[\left(\sum_{i} d_{i} \varepsilon_{i}\right)\left(\sum_{i} c_{i} 
     \varepsilon_{i}\right)\right] \\
    &=\sigma^{2} \sum_{i} c_{i} d_{i}=\frac{-\bar{x} \sigma^{2}}{S_{x x}}
\end{align*}

$a$ 和 $b$ 两者都有$\sum_{i} w_{i} y_{i} $的形式，因此它们都是线性估计量，前边给出了它们的样本均值和方差并证实了它们是无偏的。正如已指出的，还存在利用数据估计$ \alpha $ 和$ \beta $ 的其他方法。
然而，从线性无偏估计量的角度，没有任何估计量比最小二乘估计量具有更小的样本方差，这就是{\heiti 高斯—马尔科夫定理}。

当把正态分布干扰项的假定加入上面的过程时，我们得到估计量的分布的一个完备的结果。由于 $ a $ 和 $ b $ 两者都是正态分布变量的线性函数，因而它们也都是正态分布的。其均值和方差已导出，概括起来，在正态性假设下，有:
$$
\left[\begin{array}{l}
a \\
b \end{array}\right] \sim N\left[\left[\begin{array}{l}
\alpha \\
\beta
\end{array}\right], \quad \sigma^{2}\left[\begin{array}{ll}
1 / n+\bar{x}^{2} / S_{x x} & -\bar{x} / S_{x x} \\
-\bar{x} / S_{x x} & 1 / S_{x x}
\end{array}\right]\right] $$

{\subsection{b 是 \texorpdfstring{$\beta$}{β}  的最小线性无偏估计}  }

思考：证明 $b = \sum_{i}c_{i} y_{i} $是线性无偏估计量中，方差最小的一个估计量。
\begin{myproof}
    令另一个估计量是 
    $$b^{\prime}=\sum_{i} q_{i} y_{i}=\alpha \sum_{i} q_{i}+\beta \sum_{i} q_{i} x_{i}+\sum_{i} q_{i} \varepsilon_{i}$$

    在等式两边取期望，我们可以看到，若使$ b^{\prime} $ 是无偏的，必须有$ \sum_{i}q_{i} = 0 $及 $ \sum_{i}q_{i}x_{i} =1$ 。这样，$ b^{\prime} = \beta + \sum_{i}q_{i}\varepsilon_{i} $。 $ b^{\prime} $的方差是:
    $$ \operatorname{Var}\left[b^{\prime}\right]=\sigma^{2} \sum_{i} q_{i}^{2} $$

    令$ v_{i}=q_{i}-c_{i},$  则   $ q_{i}=c_{i}+v_{i} $且
    $$ \begin{array}{c}
        \operatorname{Var}\left[b^{\prime}\right]=\sigma^{2} \sum_{i}\left(c_{i}+v_{i}\right)^{2} 
        =\sigma^{2}\left(\sum_{i} c_{i}^{2}+\sum_{i} v_{i}^{2}+2 \sum_{i} c_{i} v_{i}\right)
        \end{array} $$

    利用$ \sum_{i} q_{i} = 0 $和$\sum_{i} q_{i} x_{i} =1 $，易得到$\sum_{i} c_{i}v_{i} = 0$，这就是在$ b^{\prime} $的方差中只留下两个平方项，这意味着$Var[b^{\prime}]$一定大于$Var[b]$。

    Why  \ \ $\sum_{i} c_{i}v_{i} = 0$ ?
    \begin{align*}
        \Longrightarrow \sum_{i} c_{i} v_{i}&  =\sum_{i}\left(q_{i}-c_{i}\right) c_{i}=\sum_{i} c_{i} q_{i}-\sum_{i} c_{i}^{2} \\
         &=\frac{\sum_{i} x_{i} q_{i}-\sum_{i} q_{i} \bar{x}}{S_{x x}}-\frac{1}{S_{x x}}=\frac{1}{S_{x x}}-\frac{1}{S_{x x}}=0
    \end{align*}

\end{myproof}

\section{最小二乘估计量的统计推断}

在前面的内容里，我们在假定干扰项是正态分布和样本 $X_{1},X_{n} $是非随机的条件下， 给出了最小二乘估计量的确切的样本分布。但通常的参数估计过程包括构造置信区间和对$ \alpha $ 和$ \beta $值的假设检验。为了做到这一点，我们需要参数的真正样本方差的估计，这将需要对未知参数$ \sigma^{2} $ 的一个估计，并构造假设检验方法。

\subsection{ \texorpdfstring{$ \sigma^{2} $}{σ2 } 的无偏估计量的推导}

由于$ \sigma^{2} $是 $ \varepsilon_{i} ^{2} $的期望值，而$e_{i}$是$\varepsilon_{i} $的一个估计，
$$ \hat{\sigma}^{2}=\frac{1}{n} \sum_{i} e_{i}^{2} $$

似乎是一个自然的估计量，通过写出$ e_{i} = y_{i} − a − bx_{i}  $ ， 并把 $  y_{i}=\alpha+\beta x_{i}+\varepsilon_{i} $  $\alpha=\bar{y}-\beta \bar{x}-\bar{\varepsilon}$
 和  $ a=\bar{y}-b \bar{x} $ 代入 ，我们得到
\begin{equation}
    \begin{aligned}
        e_{i} &=\varepsilon_{i}-\bar{\varepsilon}-\left(x_{i}-\bar{x}\right)(b-\beta) \\
              &=\varepsilon_{i}-\bar{\varepsilon}-\left(x_{i}-\bar{x}\right)\left(\sum_{j} c_{j} \varepsilon_{j}\right)
    \end{aligned}
    \label{eq 4.4.1}
\end{equation}

我们对某一个别干扰项$ \varepsilon_{i} $的估计受两种因素的扭曲：所有干扰项的样本平均和我们可以归于$ \beta $ 并非完美估计这一事实所造成的影响。回忆所有干扰项是独立的，所以 
$ \operatorname{ \mathbb{E}( \varepsilon_{i}  \varepsilon_{j} )} = 0 $ 若 $i \neq j$ 。现在我们平方的两边并取期望值，可得到
$$
\begin{array}{c}
    \mathbb{E}\left[e_{i}^{2}\right]=\sigma^{2}+\dfrac{\sigma^{2}}{n}+\sigma^{2}\left(x_{i}-\bar{x}\right)^{2}\left(\sum_{j} c_{j}^{2}\right)-\frac{2 \sigma^{2}}{n} \\
    -2 \sigma^{2}\left(x_{i}-\bar{x}\right) c_{i}+\frac{2 \sigma^{2}}{n}\left(x_{i}-\bar{x}\right)\left(\sum_{j} c_{j}\right)
\end{array}$$

 在对这些项求和时，我们利用 $\sum_{i} c_{i} = 0, \sum_{i} (x_{i} − x)c_{i} = 1 $ 和 $\sum_{i}c_{i} ^2 =\dfrac{ 1 }{S_{xx} } $ 。整理后，我们有
$$ E\left[\sum_{i} e_{i}^{2}\right]=(n-2) \sigma^{2} $$

这表明$ \sigma^{2} $ 的一个无偏估计量是:
$$ s^{2}=\frac{\sum_{i} e_{i}^{2}}{n-2} $$

这样，我们可以得到 b 的抽样方差的一个估计为:
$$\text { Est.Var }[b]=\frac{s^{2}}{S_{x x}}$$

以后，我们将用记号 $\text { Est.Var }[\cdot]$表示一个估计量的抽样方差的一个样本估计。

{\heiti t分布统计量的构造}
\begin{equation}
    z=\frac{b-\beta}{\sqrt{\sigma^{2} / S_{x x}}}
    \label{eq 4.4.2}
\end{equation}

的分布是标准正态。由$ \dfrac{SSE}{\sigma^2}$ 服从$ \chi^{2} (n − 2) $ 
\begin{equation}
    \frac{(n-2) s^{2}}{\sigma^{2}} \sim x^{2}(n-2)
    \label{eq 4.4.3}
\end{equation}
并且和 b 是独立的。

根据 \ref{eq 4.4.2} 和 \ref{eq 4.4.3}，我们得到：
$$ t=\frac{b-\beta}{\sqrt{s^{2} / S_{x x}}} $$

是一个标准正态变量和一个除以其自由度的卡方量的平方根之比，它服从自由度为 ($ n - 2 $)  的 $ t $  分布。这样，记 $s_{b}=\dfrac{S}{\sqrt{S_{x x}}}=\dfrac{S}{S_{x}}$
\begin{equation}
    \frac{b-\beta}{s_{b}} \sim t[n-2]
    \label{eq 4.4.4}
\end{equation}

可以形成统计推断的基础。

\subsection{抽样分布}
$ \beta $ 的置信区间将以\ref{eq 4.4.4}为基础。特别的，我们可以有
$$P\left(b-t_{\frac{\lambda}{2} } s_{b} \leqslant \beta \leqslant b+t_{\frac{\lambda}{2}} s_{b}\right)=1-\lambda$$

其中$ 1− \lambda $ 是要求的置信水平， $ t_{\lambda / 2} $ 是来自于自由度为$ (n - 2) $的 $ t $ 分布的适当的临界值。利用 $ a $ 及其估计方差，可以同样地构造$ \alpha $ 的置信区间。

\subsection{ \texorpdfstring{$\beta$} 的假设检验}
我们也可以构造干扰项方差$\sigma^{2} $的置信区间，利用 \ref{eq 4.4.3}和前边的同样推理，我们得到$ \sigma^{2} $的 95\%置信区间是:
$$\left [ \dfrac{(n-2) s^{2}}{\chi_{0.975}^{2}} , \dfrac{(n-2) s^{2}}{\chi_{0.025}^{2}} \right  ] $$

一个相关的过程是检验参数是否取一给定值，为了检验假设:
$ H_{0}: \beta = \beta^{0} ; H_{1}: \beta \neq \beta^{0}$

最简单的过程是利用我们的置信区间，置信区间给出了在给定样本数据情况下， $ \beta $ 的一个似乎可能的值的集合，如果这个集合不包含$ \beta^{0} $，则原假设应该被拒绝。在原假设下，比率:
$$ t=\frac{b-\beta^{0}}{s_{b}} $$

服从自由度为$(n - 2)$的 t 分布，其均值为 0 。这个比率在任何尾部的极端值都将使假设值得怀疑。这样，一般地，若
$$ \frac{\left|b-\beta^{0}\right|}{s_{b}} \geq t_{\frac{\lambda}{2}} $$

我们将拒绝$ H_{0} $。这里， $t_{\frac{\lambda}{2}}  $ 是来自于自由度为$(n - 2)$ 的 t 分布的 $ 100(1 - \lambda /2)$ \%临界值。 
\begin{myexample}
    在前边的回归中，我们得到 $ a = - 67.5806 $ 和 $ b = 0.9793$. 为了计算标准误差，我们需要
$$    \begin{array}{c}
        s^{2}=\dfrac{537.00}{8}=67.125  , \ \ S_{x x}=67,192.45 \\
        \bar{x}=679.24 ,\ \  s_{a}=27.91 , \ \ s_{b}=27.91
    \end{array} $$

    对一个自由度为 $ n - 2 = 8 $ 的分布， 95\%临界值是 2.306。所以，$ \alpha $ 和$  \beta $的 95 \%置信区间分别是
$$ - 67.5806 + 2.306(27.91 ) ; \ \  0.9793+2.306 (0.03161) $$ 

我们得到基于自由度为$ (10—2)  =8 $的 $\chi^{2}$ 分布的 $ \sigma^{2} $ 的置信区间， 相应的临界值是 2.18 和 15.5，所以置信区间是
$$ (10-2) \frac{67.125}{17.54}<\sigma^{2}<(10-2) \frac{67.125}{2.18} \ \ or \ \  30.62<\sigma^{2}<246.33 $$

这可能显得太宽了。然而，我们通常对$ \varepsilon $  的标准差比对其方差更感兴趣。基于同样这些 结果的$ \sigma $ 的 95\%置信区间是 5.89 至 15.69。
\end{myexample}

\section{预测}
除了参数的估计外，回归的最常见的作用是进行预测。假定 $x^{0}$ 是回归量的已知值，且我们对预测与 $x^{0}$ 相应的 $y$ 的取值 $y^{0}$ 感兴趣。我们将试图对真值 $ y^{0} $进行预测：

\subsection{个体预测(Individual Prediction）}
$$y^{0}=\alpha+\beta x^{0}+\varepsilon^{0}$$

预测值将是 $\hat{y}^{0}=a+b x^{0} \quad, \quad\left(\varepsilon^{0} \sim N\left(0, \sigma^{2}\right)\right.$
且 $\left.\mathbb{E} [ \varepsilon^{0} \varepsilon_{i} \right ] =0, \quad \mathrm{i}=1, \ldots, \mathrm{n}$

预测误差是：
$$ \begin{aligned}
e^{0} &=y^{0}-\hat{y}^{0} \\
&=\alpha+\beta x^{0}+\varepsilon^{0}-a-b x^{0} \\
&=(\alpha-a)+(\beta-b) x^{0}+\varepsilon^{0}
\end{aligned} $$

在两边取期望有 $ \mathbb{E}[e^{0}] = 0 $。所以，在预测误差均值为 0 这个意义上最小二乘预测是无偏的。
预测误差的方差是:
$$ \begin{aligned}
    \operatorname{Var}\left[e^{0}\right] &=\operatorname{Var}[a]+\left(x^{0}\right)^{2} \operatorname{Var}[b]+2 x^{0} \operatorname{Cov}[a, b]+\operatorname{Var}\left[\varepsilon^{0}\right] \\
    &=\sigma^{2}\left[1+\frac{1}{n}+\frac{\bar{x}^{2}}{S_{x x}}+\frac{\left(x^{0}\right)^{2}}{S_{x x}}-\frac{2 \bar{x} x^{0}}{S_{x x}}\right] \\
    &=\sigma^{2}\left[1+\frac{1}{n}+\frac{\left(x^{0}-\bar{x}\right)^{2}}{S_{x x}}\right]
\end{aligned} $$

所以$\dfrac{y^{0}-a-b X^{0}}{\left.\sigma \sqrt{\left(1+\frac{1}{n}+\frac{\left(X^{0}-\bar{X}\right)^{2}}{S_{x x}}\right.}\right)} \sim N(0,1)$，又因为 
$ \dfrac{(n-2) s^{2}}{\sigma^{2}} \sim \chi^{2}(n-2) $。

所以 $ \dfrac{y^{0}-a-b X^{0}}{\left.s \sqrt{\left(1+\frac{1}{n}+\frac{\left(X^{0}-\bar{X}\right)^{2}}{S_{x x}}\right.}\right)} \sim t(n-2) $

我们能够为 $ y^{0} $ 构造一个预测区间，它具有和个别参数置信区间相同的形式，特别地，我们的预测区间将是:
\begin{equation}
    \left(a+b x^{0}\right) \pm t_{\lambda / 2} \sqrt{s^{2}\left[1+\frac{1}{n}+\frac{\left(x^{0}-\bar{x}\right)^{2}}{S_{x x}}\right]}
\end{equation}

\subsection{均值预测（Mean Prediction）}
均值预测是预测值是 $ y^{0}=\alpha+\beta x^{0} $ 而不考虑随机干扰项

预测误差是:
$$ \begin{aligned}
    e^{0}  &= y^{0}-\hat{y}^{0} \\
    &= \alpha+\beta x^{0}-a-b x^{0} \\
    &=(\alpha-a)+(\beta-b) x^{0}
\end{aligned} $$

在两边取期望有 $ \mathbb{E}[e^{0}]=0 $。所以，在预测误差均值为 0 这个意义上最小二乘预测是无偏的。预测误差的方差是:
\begin{align*}
    \operatorname{Var}\left[e^{0}\right] & = \operatorname{Var}[a]+\left(x^{0}\right)^{2} \operatorname{Var}[b]+2 x^{0} \operatorname{Cov}[a, b] \\ 
    & = \sigma^{2}\left[\frac{1}{n}+\frac{\bar{x}^{2}}{S_{x x}}+\frac{\left(x^{0}\right)^{2}}{S_{x x}}-\frac{2 \bar{x} x^{0}}{S_{x x}}\right] \\ 
    & = \sigma^{2}\left[\frac{1}{n}+\frac{\left(x^{0}-\bar{x}\right)^{2}}{S_{x x}}\right]
\end{align*}

所以$\dfrac{y^{0}-a-b X^{0}}{\sigma \sqrt{\left(\frac{1}{n}+\frac{\left(X^{0}-\bar{X}\right)^{2}}{S_{x x}}\right.}} \sim N(0,1)$，
又因为$\dfrac{(n-2) s^{2}}{\sigma^{2}} \sim \chi^{2}(n-2)$

所以$\dfrac{y^{0}-a-b X^{0}}{s \sqrt{\left(\frac{1}{n}+\frac{\left(X^{0}-\bar{X}\right)^{2}}{S_{x x}}\right)}} \sim t(n-2)$
\vspace{0.5em}
我们能够为$ y^{ 0 } $ 构造一个预测区间，它具有和个别参数置信区间相同的形式，特别地，我们的预测区间将是
\begin{equation}
    \left(a+b x^{0}\right) \pm t_{\lambda / 2} \sqrt{s^{2}\left[\frac{1}{n}+\frac{\left(x^{0}-\bar{x}\right)^{2}}{S_{x x}}\right]}
\end{equation}

\begin{myexample}
    如果 1980 年的可支配收入预测是 1030 美元（十亿），为了计算一个预测区间，我们需要
   $ a=-67.5806 ,\ \
    b=0.9793 ,\ \
    s^{2}=67.125 ,\ \
    \bar{x}=879.24 ,\ \
    S_{x x}=67,192.44 ,\ \
    n=10 $

    $ t $ 分布的临界值是 2.306，将这些代入 3 得到一个预测区间是：
    $-67.5806 \pm  0.9793(1030) \pm  2.306(9.8256) $
    $\ \  i.e. 941.1 \pm 22.658 $ 。
\end{myexample}